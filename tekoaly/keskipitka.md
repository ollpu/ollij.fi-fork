# Keskipitkä versio

Kerron tässä, minkä vuoksi pidän kohtalaisen todennäköisenä, että tekoäly aiheuttaa eksistentiaalisen katastrofin.

Tietämättä parempaa tapaa kertoa aiheesta, päädyn luettelemaan erinäisiä uskomuksiani aiheeseen liittyen.

1\. *Tekoälyjärjestelmät pystyvät olemaan valtavan paljon kyvykkäämpiä kuin ihmiset oleellisesti missä tahansa kognitiivisissa tehtävissä.*

(Arkisempi muotoilu "tekoälyt voivat olla paljon ihmisiä älykkäämpiä" kärsii sanasta "älykkyys", joka aiheuttaa eri ihmisissä hyvin erilaisia reaktioita, välillä johtaen ajattelun sivuraiteille.)

Ihmisten kyvyt asettavat *alarajan*, ei ylärajan, sille, kuinka kyvykkäitä tekoälyt voivat olla. Ihmisaivojen suorittamaa työtä voi toistaa tietokoneella. Tietokoneilla ei ole samankaltaisia rajoituksia koskien suoritetun laskennan määrää, informaation prosessointinopeutta tai skaalautuvuutta kuin ihmisillä.

Niin ikään tekoälyt voivat olla *laadullisesti* parempia kuin ihmiset, samalla tavalla kuin ihmiset ovat laadullisesti kyvykkäämpiä kuin muut eläimet tai jotkin ihmiset ovat valtavan paljon muita parempia yksittäisissä tehtävissä. Ero ei johdu siitä, että ihmiset tekisivät huomattavasti enemmän ajattelua kuin muut eläimet. Ero syntyy siitä, *millaisia* ajatuksia tai millaista laskentaa tekee.

Suuri osa tekoälyalasta keskittyy kapeisiin tehtäviin (kuten "miten voittaa ihminen gossa"). Silti tekoäly voi olla *yleisesti* älykäs -- samassa mielessä kuin ihmiset kykenevät miettimään koko ympärillä olevaa maailmaa (tai maailmankaikkeutta), ratkomaan hyvin laajaa joukkoa erilaisia ongelmia ja reflektoimaan omia ajatuksiaan. Ihmisten olemassaolo demonstroi, että tämä on mahdollista. Samanlaisen mekanismin voi toteuttaa tietokoneilla.

2\. *Ihmisiä valtavan paljon kyvykkäämpi tekoäly kykenee aiheuttamaan ihmiskunnan tuhon (tätä tavoitellessaan)*

Tämä väite vaikuttaa jakavan ihmisiä: jotkut pitävät sitä itsestäänselvyytenä, jotkut taas ovat vahvasti eri mieltä. Päädyn siksi selittämään näkemyksistäni yleisten vastareaktioiden kautta.

Usein ihmiset haluavat jonkin *yksittäisen tarinan*, miten tällainen valloitus tapahtuisi. Konkreettiset tarinat eivät kuitenkaan ole isoin syy, minkä takia uskon väitteeseen. Minusta se vaikuttaa aika ilmeiseltä, että hyvin älykkäät tekoälyt voivat tehdä hurjia asioita! Selvästi niiden kanssa tulee olla varovainen!

Intuition lähteitä:
- Aikuiset ihmiset kykenevät aika paljoon sellaiseen, mihin pienet lapset (tai muut eläimet) eivät kykene.
- Jos voisin ajatella tuhatkertaisella nopeudella -- samaan tapaan kuin tietokone-ohjelman saa ajamaan nopeammin lisäämällä laskentatehoa -- eli jos muu maailma näyttäytyisi minulle tuhat kertaa hitaammin, pystyisin aika paljoon sellaiseen, mihin en nyt kykene.
- Pelatessani shakkia maailmanmestaria vastaan *tiedän*, että tulen häviämään, vaikken tiedä *miten*.
- Ihmiskunta nykyään kykenee aika paljoon sellaiseen, mikä olisi muutama vuosisataa sitten näyttänyt taikuudelta.

Pitkälti odotan kehittyneiden tekoälyjen tekevän asioita, joita minä tai muut ihmiset eivät yksinkertaisesti osaa ennakoida etukäteen. Kyvykkäät tekoälyt ajattelevat ajatuksia joita ihmiset eivät.

Tästä näkökulmasta on luontevampaa kysyä syitä sille, miksi kehittyneet tekoälyt *eivät* kykenisi toteuttamaan tavoitteitaan.

Yksi vastaus on, että ihmiset ja ihmisjoukot yrittävät (ja ovat yrittäneet) saada valtaa, ja kuinka mikään taho ei ole onnistunut "maailmanvaltauksessa". En pidä tätä kovin vahvana vasta-argumenttina. Helppo vastaus: *ihmiskunta* on onnistunut maailmanvalloituksessa. Toinen vastaus: trendin ei tule olettaa jatkuvan, jos sen takana olevat kausaaliset mekanismit merkittävästi muuttuvat -- samaan tapaan kuin "kukaan ihminen ei ole ollut ylivoimainen gossa muihin nähden, joten tekoälykään ei tule olemaan ylivoimainen gossa ihmisiin nähden" on huono päätelmä. Trendin ekstrapoloimisen sijasta tulee miettiä tekoälyn kehityksen vaikutuksia.

Toinen vastaus on puhua siitä, kuinka tekoäly on lopulta vain tietokoneohjelma, eikä se voi tehdä asioita "oikeassa maailmassa". Nämä ajatukset vaikuttavat pohjautuvan huonoihin abstraktioihin. Erottelu "digitaalisen maailman" ja "fyysisen maailman" välillä ei ole aito. Tekoälyt tietenkin vaikuttavat maailmaan: vaikka ne esimerkiksi "vain tuottaisivat tekstiä", niin välillä ihmiset tekevät asioita tekstin lukemisen seurauksena ja tuotettu teksti siten johtaa oikean maailman seurauksiin. Tekoälyjä kehitetään nimenomaan sen takia, että niiden avulla saadaan tehtyä asioita. (Käytettävyyden ja tuottavuuden näkökulmasta voisi myös odottaa, että tekoälyt "laitetaan tekemään" enemmän ja enemmän asioita ilman, että ihmiset toimivat välikätenä. Tämä tietysti on se, mitä tällä hetkellä tapahtuu.)


3\. *Tavoitteiden asettaminen tekoälyihin on avoin ongelma*

Emme yksinkertaisesti osaa asettaa tavoitteita tekoälyihin. Yksi tarkempi muotoilu tämän korkean tason väitteen taustalla:

*Syväoppimismallien kouluttaminen jonkin häviöfunktion (engl. loss function) suhteen ei johda malleihin, jotka sisäisesti "välittävät" tästä tavoitteesta.*

Usein tämä idea selitetään tutkimalla vastaavaa tilannetta evoluution ja ihmisten välillä. Evoluutio optimoi lajeja karkeasti lisääntymiseen ja geenien levittämiseen, tarkemmin *kelpoisuuteen* (engl. fitness). Tämä ei kuitenkaan ole ihmisten tavoite! Ihmiset välittävät asioista, jotka liittyvät tai *ovat liittyneet* geenien leviämiseen, ei siitä *itsestään*. Ihmiset tykkäävät sokerista ja syövät makeita asioita, vaikka ne olisivat evoluution näkökulmasta nykyään haitallisia. Ylipäätään ihmisten tavoitteeet ovat hyvin monenlaisia, ja iso osa niistä on irrallisia lisääntymisestä.

Tyypillisesti tekoälymalleja koulutettaessa laajasta joukosta mahdollisia malleja etsitään parametreja muuttamalla sellaisia, jotka pärjäävät hyvin jonkin tietyn häviöfunktion suhteen. Ajankohtainen esimerkki on suuret kielimallit, joissa mittarina on (karkeasti) "kuinka hyvin malli ennustaa tekstiä". Tämä ei kuitenkaan johda malliin, joka sisäisesti "tavoittelee" tekstin ennustamista hyvin -- aivan kuten evoluution "etsiessä" biologisia organismeja mittarinaan geenien levittäminen eivät kyseiset organismit sisäisesti tavoittele geeniensä levittämistä.

Sama pätee myös vahvistusoppimisen (engl. reinforcement learning) kontekstissa, jossa häviöfunktion sijaan puhutaan palkkioista (engl. reward). Hämäävästä nimestään huolimatta palkkio *määrittelee prosessin, jolla parametreja muutetaan*, eikä itsessään ole asia, jota malli sisäisesti tavoittelee. Palkkion antaminen vahvistaa tietyntyyppistä toimintaa ("älä tee noin" tai "tee enemmän tuon tyyppisiä juttuja"). Epätäydellinen analogia ihmisiin: palkkiot vastaavat kivun tai mielihyvän signaaleja, *eivät* ihmisten arvoja.

Yleisesti syväoppimisparadigmassa ei ole tapaa saada *sisäisiä ominaisuuksia* malleihin, vaan optimointi kohdistuu annettuihin tuloksiin. Tekoälyä voi kouluttaa tavoitteella "ennusta tekstiä hyvin", jolloin mallin *käytös* vastaa hyvää tekstin ennustamista. Sen sijaan mallin sisäiset ominaisuudet jäävät tuntemattomiksi.

Vaikka siirryttäisiin syväoppimisen ulkopuolelle, niin ihmiskunta ei tällä hetkellä osaa suunnata tekoälyjärjestelmiä kohti (oikean maailman) tavoitteita. Kielimalleilla on kohtalaisen hyvä käsitys esimerkiksi ihmisten arvoista, mutta *ymmärrys* ei kuitenkaan tarkoita sitä, että malli *välittäisi*. Emme osaa rakentaa järjestelmää, joka välittäisi, järjestelmää, joka oikeasti tavoittelisi ihmisten arvojen toteutumista.

(Nopeat reaktiot kuten "voimmehan rakentaa järjestelmän, joka kysyy kielimallilta parasta toimintatapaa ihmisten arvojen näkökulmasta ja joka sitten tekee niin" missaavat pointin. Yksi ongelma on, että tällainen järjestelmä optimoi *kielimallin käsitystä* ihmisten arvoista, ei ihmisten arvoja. Emme taaskaan osaa suunnata tekoälyjä kohti niitä *oikeita asioita*, ainoastaan johonkin "vähän sinne päin".)

Monesti keskustelu tekoälyuhasta (etenkin teknisen yleisön ulkopuolella) juuttuu kysymykseen "millaisia arvoja tekoälyyn kannattaisi laittaa?". On toki vaikea ongelma valita sellaiset tavoitteet, että niiden optimoiminen *kovaa* -- ihmiset ylittävällä tasolla -- johtaa hyviin lopputuloksiin. Vielä keskeisempi ongelma on kuitenkin se, *miten nämä tavoitteet ylipäätään saadaan laitettua tekoälyyn*. Vaikka pystyisimme spesifioimaan tarkalleen, millaiset ovat "hyvät arvot", iso osa ongelmaa on edelleen ratkomatta: miten saada nämä hyvät arvot kytkettyä tekoälyyn? Haaste ei ole luonteeltaan moraalifilosofinen, tämä on *tekninen ongelma*, johon meillä ei ole ratkaisua.

4\. *Pelkästään tekoälyjen toiminnan tarkastelu on riittämätöntä*

Kategorian "kerätään ihmisiltä palautetta siitä, kuinka hyvää mallin toiminta on ja muutetaan mallia palautteen perusteella niin, että se toimii paremmin" lähestymistavat törmäävät tähän ongelmaan: sanottaessa mallille "älä tee noin" saadaan malli ainoastaan *toimimaan tavalla, jossa ihmiset eivät näe ongelmia*.

Siis: Toiminnan generoivaa prosessia muutetaan, kunnes näkyviä ongelmia ei ole.

Tämä ei saa mallia sisäistämään ja jakamaan ihmisten tavoitteita. Ihmisten arvot eivät astu kehiin missään vaiheessa. Toiminnan generoivaa prosessia muutetaan niin, että se tuottaa enemmän hyväksi merkittyjä esimerkkejä. Mitä tekemistä tällä on ihmisten arvojen kanssa? Ihmisten palaute tietyssä kontekstissa ei ole sama asia kuin heidän oikeat arvonsa. Ei pelkästään siksi, että palaute ei täysin heijasta arvoja, vaan ennen kaikkea siksi, että nämä ovat täysin eri *tyyppisiä* asioita.

Ja jos mallia "rankaistaan" huonosti toimimisesta (tarkemmin: mallia muutetaan niin, että se tekee vähemmän näiden tapausten mukaisia asioita), niin tämä ohjaa mallia toimimaan niin, ettei huonoa toimintaa näy. Kannustin on kohti huonon toiminnan piilottamista, niinpä huono toiminta piiloutuu. Ihminen voisi kuvailla tätä niin, että tekoäly päätyy huijaamaan ihmisiä, mutta tietoista yritystä huijata ei tarvita: jos koulutat mallilta pois näkyvän huonon toiminnan, niin "huonoa toimintaa on edelleen, mutta se on piilossa" on varsin ennustettava lopputulos.

Emme saa mallia toimimaan ihmisten arvojen mukaisesti, saamme mallin toimimaan tavalla missä ihmiset eivät näe ongelmia.

(Taas, emme osaa asettaa malleille tavoitteita.)

Ongelmia tulee vastaan myös puhtaasti mallin *toiminnassa*, ei vain sisäisissä ominaisuuksissa. Palaute ei täysin heijasta arvoja. Ihmiset tekevät virheitä. Eivätkä pelkästään "sattumalta", vaan eri ihmiset tekevät samoissa asioissa samantyyppisiä virheitä. Mallin muuttaminen ihmisten palautteen perusteella saa mallin toimimaan tällä virheellisellä tavalla. Jos esimerkiksi ihmiset eivät mieti toimintansa pidemmän ajan seurauksia (tai eivät kykene hahmottamaan näitä seurauksia riittävän hyvin), seuraukset ovatkin pitkällä tähtäimellä huonoja, ja jos mallia muutetaan tekemään tämäntyyppisiä asioita, niin...

Jälleen kerran: Koulutusprosessi ei johda ihmisten arvojen mukaiseen toimintaan, se johtaa ihmisten palautteen mukaiseen toimintaan.

(Tai oikeastaan palautteen mukaiseen toimintaan *koulutusympäristössä*. Miten malli yleistyy uusiin tilanteisiin? Jaa-a. Mallien yleistymistä ymmärretään huonosti. Vastaus "juuri siten, miten toivoisi" ei kuitenkaan ole oikea.)

Nämä hienovaraiset erot ihmisten arvojen ja ihmisten palautteen välillä, oikean toiminnan ja tavoitteiden sisäistämisen välillä, koulutusympäristön toiminnan ja sen ulkopuolisen toiminnan välillä, ovat aidosti tärkeitä. Kuvauksesta "malli oppii ihmisten palautteesta, mistä ihmiset tykkäävät" voisi ajatella, että kaikki on hyvin -- ja eikö tämä kuvaakin sitä, mitä prosessissa tapahtuu?  Tämä on kuitenkin virheellinen selitys siitä, mitä palautetta antaessa oikeasti tapahtuu, "vähän sinne päin", piilottaen yllä käsitellyt ongelmat.

5\. *Ihmiskunnalla on hyvin heikko käsitys siitä, mitä syväoppimismallien sisällä tapahtuu*

Prosessi syväoppimismallien kouluttamisen taustalla on karkeasti: "Aloitetaan satunnaisesta mallista. Katsotaan, kuinka hyvin se pärjää muutamassa esimerkkitilanteessa. Muutetaan sitä (automaattisesti) niin, että se olisi toiminut näissä tapauksissa paremmin. Toistetaan." Käytännössä on todettu, että tällaisten prosessin tuloksina on malleja, jotka pärjäävät hyvin halutulla mittarilla.

Oleellinen pointti: Ihmisillä, jotka käynnistävät tämän prosessin tietokoneillaan (tai laskentakeskuksillaan), ei ole etukäteen tietoa siitä, millainen lopputulos on. Se tiedetään, että se pärjää hyvin. Sitä ei tiedetä, *miten* tai *miksi* se pärjää niin hyvin.

"Emme tiedä, miten mallit toimivat" kuvaa tilannetta hyvin -- tiedämme kirjaimellisesti mitään, mutta hyvin vähän. Olemme kaukana vastauksista korkean tason kysymyksiin kuten "Mitä mallin sisällä tapahtui, kun se antoi tuon vastauksen? Miten se päätyi tulokseensa? Miksi se teki noin eikä näin?"

6\. *Ongelman haastavuus kasvaa tekoälyjen kehittyessä*

Ihmisiä älykkäämpien tekoälyjen tapauksessa ei voi laskea esimerkiksi sen varaan, että ihmiset pystyvät arvioimaan, onko tekoälyn toiminta "hyvää" vai "huonoa". Kyvykkäät tekoälyt pystyvät johtamaan ihmisiä harhaan, ihmiset eivät kykene hahmottamaan tekoälyn toiminnan seurauksia ja tekoäly keksii asioita, joita ei itse keksi.

Yleisemmin: Kyvykkäät tekoälyt kykenevät tekemään asioita, joihin vähemmän kyvykkäät eivät.

Tämän seurauksena uusia ongelmia ilmaantuu mallien kehittyessä. Kenties jotkin ongelmat ilmaantuvat vasta niiden muodostaessa jo merkittävän riskin uhkaskenaarioille. Näitä haasteita tulee ennakoida etukäteen. Jälkikäteen reagointi vain empiirisesti todettuihin ongelmiin ja iteratiiviset lähestymistavat ongelmien poistamiseksi eivät riitä. (Käytännössä taas uusien kykyjen ilmaantuminen kielimalleihin on ennalta-arvaamatonta.)

7\. *Ongelman ratkaisemiseen on vain rajallinen määrä aikaa*

On useampia organisaatioita, jotka eksplisiittisesti yrittävät rakentaa yleistekoälyn. Ajankohtien ennustaminen on vaikeaa, mutta näyttää kuitenkin hyvinkin mahdolliselta, että nämä organisaatiot onnistuvat tässä lähitulevaisuudessa. Tekninen ongelma tulee ratkaista tätä ennen.

---

Miltä uhkaskenaariot näyttävät?

Ajatukseni ovat ennemminkin muotoa "kehittyneet tekoälyt ovat lähtökohtaisesti vaarallisia ja taustalla oleva tekninen ongelma on haastava" kuin tarkkoja ennustuksia siitä, miltä tulevaisuus tulee näyttämään. Tulevaisuuden ennustaminen on vaikeaa. On useamman tyyppisiä uhkaskenaarioita, joita pidän ihan mahdollisina.

Keskeinen epävarmuuteni koskee "lähtönopeuksia" (engl. takeoff speeds): kuinka jatkuvaa tekoälykehitys on? Puhuessani kehityksen jatkuvuudesta tarkoitan nimenomaan *jatkuvuutta*, en niinkään *nopeutta*. (Tietokoneiden laskentateho on kasvanut viime vuosikymmeninä nopeasti (eksponentiaalisesti), mutta kasvu on silti ollut ennustettavaa ilman suuria hyppyjä. Sen sijaan ydinaseiden kehitys johti epäjatkuvuuteen räjähdeaseiden voimakkuudessa.)

Jos kehitys on hyvin epäjatkuvaa, eli jossakin kohtaa uusin koulutettu malli on valtavan paljon edellisiä (ja ihmisiä) parempi, näkisin uhkaskenaarioiden olevan muotoa "kaikki ihmiset kuolevat yhtäkkiä" -- tai jotakin muuta, missä ihmiset eivät voi jälkikäteen tehdä uhalle mitään (jos he edes ovat siitä tietoisia).

(Yksi syy, miksi odottaa valtavaa hyppyä kehityksessä: Ihmisten kognitiiviset kyvyt, tai ainakin niiden vaikutukset maailmaan, ovat kehittyneet hyvin nopeasti evolutiivisella aikajänteellä. Satoja ja satoja miljoonia vuosia evoluutiota... kunnes muutaman kymmenen tuhannen vuoden aikana kaikki kääntyy päälaelleen, evoluutio ei olekaan enää dominantti voima, maailma muuttuu vuosien eikä miljoonien vuosien aikaskaalalla. Tämä saa harkitsemaan, että yleisälykkyyden luonne on lopulta melko binäärinen: kun asiat alkavat toimimaan, ne toimivat todella hyvin.)

(Toinen huomio: AlphaGo -sarjan mallit oppivat pelaamaan gota päivien aikaikkunassa paremmin kuin mitä kukaan ihminen lajin vuosituhansia pitkän historian aikana. Kehitys voi olla ihmisten mittapuulla hyvin nopeaa -- niinkin nopeaa, ettei ihmiset kykene reagoimaan siihen.)

(Miten tällainen tekoäly tarkalleen ottaisi kontrollin tai yhtäkkiä tappaisi kaikki? En tietenkään tiedä tarkkaa väylää. Keksin toki joitakin reittejä, joita voi hyödyntää -- ja joita itse voisin hyödyntää, jos vaikkapa subjektiivinen aikani kuluisi kymmenpotensseja nopeammin ja pyrkisin tähän tavoitteeseen. Pitkälti tosin odotan, että samaan tapaan kuin monet nykyhetken teknologiat näyttävät taikuudelta tuhannen tai parin sadan vuoden takaisille ihmisille, tällainen hyvin kehittynyt tekoäly keksii asioita, joita minä en kykene keksimään.)

Jos taas kehitys on jatkuvampaa, niin... no, jos tekoälyjä silti vain kehitetään eteenpäin, samat riskit tulevat vastaan, kun ennen pitkää tekoälyistä tulee paljon ihmisiä kyvykkäämpiä. Verrattain pienet erot eri tekoälyjen välillä ei poista riskiä, etenkään jos näitä tekoälyjä ei käytä ongelman ratkaisemiseksi.

Jatkuvamman kehityksen tapauksessa näen tosin myös mahdollisena, että riskit toteutuvat eri tavalla.

Yksi laaja kategoria uhkakuvia: Tekoälyjä kehitetään, ihmiset ottavat tekoälyjä laajemmin käyttöön, näillä tekoälyillä automatisoidaan enemmän ja enemmän asioita, yhä laajempi osuus tehtävistä on yhä enemmän autonomisten tekoälyjen suorittamia. Ihmisten ymmärrys ja kontrolli hiljalleen vähenee. Tekoälyt tekevät asioita, jotka näyttävät hyviltä, mutta jotka ovat lopulta vain "vähän sinne päin": lukemat mittareilla näyttävät hyviltä, mittari ei vastaa tarkalleen mitä haluamme, todellisuudessa asiat eivät ole hyvin. On erimielisyyttä siitä, kuinka merkittäviä nämä ongelmat ovat. Riippuvuus tekoälyistä, taloudelliset paineet ja teknisten ongelmien haastavuus tekee vaikeaksi tehdä asialle mitään.

Toisaalta *instrumentaalinen suppeneminen* (engl. instrumental convergence) -- idea "monet välitavoitteet (kuten resurssien hankinta) ovat hyödyllisiä riippumatta lopputavoitteesta" -- soveltuu edelleen. Tekoälyjärjestelmät, tai ainakin osa niistä, pyrkivät saamaan resursseja ja vaikutusvaltaa. Tulevaisuuden suunta määräytyy enenevissä määrin näiden tekoälyjen, ei ihmisten, toiminnan perusteella.

(Miten tarkalleen eksistentiaalinen katastrofi toteutuisi? Tämä kategoria on hyvin laaja ja disjunktiivinen -- voi tapahtua näin tai noin tai näin -- ja mikään spesifi tarina ei vaikuta minusta erityisen todennäköiseltä. "Tekoälyt jatkavat kehitystään, ihmisten ymmärrys ja kontrolli heikkenee" vain vaikuttaa tilanteelta, jossa on paljon riskejä ja johon ei haluta päätyä.)

Huomautan, että eri ihmiset pitävät eri uhkakuvia todennäköisinä ja epätodennäköisinä. (Ei sillä, että tämä on välttämättä hyvä asia. Päinvastoin erimielisyys ja tietämättömyys tulevasta vaikuttaa huonolta.) Jotkut eivät pidä uhkia ylipäätään todennäköisinä.

Erimielisyydet eivät välttämättä ratkea lähitulevaisuudessa. Uhkakuvat eivät vaadi "varoituslaukauksia" tai pienempiä onnettomuuksia ennen suuria katastrofeja. Monissa skenaarioissa niistä asiat *näyttävät* hyviltä ja siten ei ole yksittäistä hetkeä, joka herättää ihmiset riskeihin.

Itse pidän todennäköisenä, että tekoäly aiheuttaa eksistentiaalisen katastrofin eli kaikkien ihmisten kuoleman tai muuten ihmiskunnan kehityksen katkeamisen. En anna tarkkoja lukemia, jotta en anna väärää kuvaa uskomusteni tarkkuudesta, mutta todennäköisyyteni näille lopputuloksille on yli 50 prosenttia. (Tässä numeerinen todennäköisyys kuvastaa yksinkertaisesti minun uskomusteni vahvuutta, kuten olen [toisaalla](/epi/probabilistinen_ajattelu) kirjoittanut.)

Taas, näkemykseni eivät perustu tarkkoihin ennustuksiin lähitulevaisuuden tapahtumista. Matkalla voi tapahtua yllättäviä asioita. Näen kuitenkin tekoälyuhan *konvergenttina*, lopputulokselta jota kohti sivilisaatiomme on lähtökohtaisesti ajautumassa, enkä epätodennäköisenä sivuhaarana. (Edelleen, lukuisat organisaatiot yrittävät rakentaa yleistekoälyn, laskentakapasiteetin kasvaessa ajan myötä tämän saavuttaminen muuttuu helpommaksi ja tulevaisuudessa parhaat tekoälyt eivät tule olemaan huonompia kuin parhaat vuonna 2023.) Uhan välttäminen vaatii muutosta ihmiskunnan oletuksena olevaan kulkusuuntaan.

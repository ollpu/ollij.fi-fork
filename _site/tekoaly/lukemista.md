# Lukemista

Uskomukseni koskien tekoälyuhkaa nojaa vahvasti muiden ihmisten kirjoituksiin aiheesta. Kokoan alla joitakin tekstejä, jotka ovat vaikuttaneet ajatuksiini ja joista löytää aiheesta enemmän.

Teksti [AI Alignment, Explained in 5 Points](https://medium.com/@daniel_eth/ai-alignment-explained-in-5-points-95e7207300e3) (Daniel Eth) on kenties paras yleisluontoinen selitys ongelmasta, jonka olen nähnyt.

Koskien itse teknistä ongelmaa, Eliezer Yudkowskyn [AGI Ruin: A List of Lethalities](https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) ja Paul Christianon [Where I agree and disagree with Eliezer](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer) ovat hyvä kuvaus joistakin yhteisistä ja eriävistä näkemyksistä alan sisällä.

Yleisesti jos haluaa lukea aiheesta lisää: [Alignment Forum](https://www.alignmentforum.org/) saattaa alkuun.

---

Joitakin lisäkommentteja ja materiaaleja, joista voi lukea lisää [keskipitkässä versiossa](/tekoaly/keskipitka) esittämistäni väitteistä:

- Väite "syväoppimismallien kouluttaminen jonkin häviöfunktion (engl. loss function) suhteen ei johda malleihin, jotka sisäisesti 'välittävät' tästä tavoitteesta"
    - Rob Miles, "[The OTHER AI Alignment Problem: Mesa-Optimizers and Inner Alignment](https://www.youtube.com/watch?v=bJLcIBixGj8)"
    - Rafael Harth, "[Inner Alignment: Explain like I'm 12 Edition](https://www.alignmentforum.org/posts/AHhCrJ2KpTjsCSwbt/inner-alignment-explain-like-i-m-12-edition)"
    - Yllä olevat materiaalit perustuvat työhön Hubinger et al., "[Risks from Learned Optimization in Advanced Machine Learning Systems](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)"
- "Sama pätee myös vahvistusoppimisen kontekstissa"
    - Ks. yllä
    - Alex Turner, "[Reward is not the optimization target](https://www.alignmentforum.org/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target)"
- Väite "ihmiskunta ei tällä hetkellä osaa suunnata tekoälyjärjestelmiä kohti (oikean maailman) tavoitteita"
    - Ks. yllä
    - (Mainitsen ohimennen termit "Ontology identification" ja "Pointers problem")
- Väite "[...] mutta *ymmärrys* ei kuitenkaan tarkoita sitä, että malli *välittäisi* [...]"
    - (Analogia: Ihmiset voivat hyvin ymmärtää muurahaisten toimintaa haluamatta kuitenkaan itse rakentaa muurahaiskekoja.)
    - (Yksi matemaattinen malli agenteille koostuu uskomuksista ja tavoitteista -- vertaa "maksimoidaan utiliteetin odotusarvoa" -- jolloin ymmärrys ja välittäminen ovat kaksi täysin eri asiaa.)
    - (Ihmisillä tuntuu olevan paljon väärinymmärryksiä ja erimielisyyksiä tähän aiheeseen liittyen -- ks. esim. [tämä teksti ja sen kommentit](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument))
- Ehdotus "[...] kysyy kielimallilta parasta toimintatapaa ihmisten arvojen näkökulmasta ja joka sitten tekee niin [...]"
    - On hyvin epäselvää, miten tämä "sitten tekee niin" -osa toteutettaisiin
- Väite "pelkästään tekoälyjen toiminnan tarkastelu on riittämätöntä turvallisuuden määrittämiseksi"
    - Ks. yllä linkatut materiaalit koskien "inner alignment" -ongelmaa, erityisesti "deceptive alignment"
- Väite "ihmiskunnalla on hyvin heikko käsitys siitä, mitä syväoppimismallien sisällä tapahtuu"
    - (On vaikea antaa viitettä sille, kuinka jotakin asiaa ei ymmärretä verrattuna siihen, että jotakin ymmärretään.)
    - (Tyydyn linkkaamaan Anthropic-organisaation artikkelin [Towards Monosemanticity](https://www.anthropic.com/index/decomposing-language-models-into-understandable-components), joka on nähdäkseni on yksi parhaista edistysaskelista tähän mennessä, antaakseni kuvan alan tilasta.)
- Koskien uhkakuvia, Paul Christianon "[What Failure Looks Like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)" kuvaa tietynlaisia skenaarioita

Iskulauseen "hyvät asiat ovat edelleen hyviä" olen mielestäni poiminut [Zvi Mowshowitzin](https://thezvi.wordpress.com/) teksteistä.
